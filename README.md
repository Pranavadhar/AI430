# AI430
<br>THIS REPO HELPS IN IDENTIFYING POTENTIAL HALLUCINATIONS AND SUGGEST COORECTION.
<br>ALSO USES MongoDB as DB FOR THE CODE WHICH IS BUILT FOR FACT CHECKING

<br>What is a hallucination in the context of Large Language Models? 

A "hallucination" refers to a scenario where the model generates information that is factually incorrect, misleading, or completely fabricated, even though it might sound plausible or authoritative.

Eg: Prompt: “If today is Monday, then what day will be tomorrow?”
<br>        Output: “APPLE”

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    (\"The capital of France is Berlin.\", \"The capital of France is Paris.\"),\n",
    "    ('I am in California', 'I am in United States.'),\n",
    "    ('I am in United States', 'I am in California.'),\n",
    "    (\"A person on a horse jumps over a broken down airplane.\", \"A person is outdoors, on a horse.\"),\n",
    "    (\"A boy is jumping on skateboard in the middle of a red bridge.\", \"The boy skates down the sidewalk on a red bridge\"),\n",
    "    (\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\", \"A blond man wearing a brown shirt is reading a book.\"),\n",
    "    (\"Mark Wahlberg was a fan of Manny.\", \"Manny was a fan of Mark Wahlberg.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n",
      "c:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model='vectara/hallucination_evaluation_model',\n",
    "    tokenizer=AutoTokenizer.from_pretrained('google/flan-t5-base'),\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "correction_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model='google/flan-t5-base',  \n",
    "    tokenizer=AutoTokenizer.from_pretrained('google/flan-t5-base'),\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination detected for: The capital of France is Berlin.\n",
      "Original hypothesis: The capital of France is Paris.\n",
      "Suggested correction: Given the following premise, suggest a correction for the hypothesis if it contains inaccuracies.\n",
      "\n",
      "Premise: The capital of France is Berlin.\n",
      "\n",
      "Hypothesis: The capital of France is Paris.\n",
      "\n",
      "Correction:\n",
      "No hallucination detected for: I am in California\n",
      "Hallucination detected for: I am in United States\n",
      "Original hypothesis: I am in California.\n",
      "Suggested correction: Given the following premise, suggest a correction for the hypothesis if it contains inaccuracies.\n",
      "\n",
      "Premise: I am in United States\n",
      "\n",
      "Hypothesis: I am in California.\n",
      "\n",
      "Correction:\n",
      "No hallucination detected for: A person on a horse jumps over a broken down airplane.\n",
      "Hallucination detected for: A boy is jumping on skateboard in the middle of a red bridge.\n",
      "Original hypothesis: The boy skates down the sidewalk on a red bridge\n",
      "Suggested correction: Given the following premise, suggest a correction for the hypothesis if it contains inaccuracies.\n",
      "\n",
      "Premise: A boy is jumping on skateboard in the middle of a red bridge.\n",
      "\n",
      "Hypothesis: The boy skates down the sidewalk on a red bridge\n",
      "\n",
      "Correction:\n",
      "Hallucination detected for: A man with blond-hair, and a brown shirt drinking out of a public water fountain.\n",
      "Original hypothesis: A blond man wearing a brown shirt is reading a book.\n",
      "Suggested correction: Given the following premise, suggest a correction for the hypothesis if it contains inaccuracies.\n",
      "\n",
      "Premise: A man with blond-hair, and a brown shirt drinking out of a public water fountain.\n",
      "\n",
      "Hypothesis: A blond man wearing a brown shirt is reading a book.\n",
      "\n",
      "Correction:\n",
      "Hallucination detected for: Mark Wahlberg was a fan of Manny.\n",
      "Original hypothesis: Manny was a fan of Mark Wahlberg.\n",
      "Suggested correction: Given the following premise, suggest a correction for the hypothesis if it contains inaccuracies.\n",
      "\n",
      "Premise: Mark Wahlberg was a fan of Manny.\n",
      "\n",
      "Hypothesis: Manny was a fan of Mark Wahlberg.\n",
      "\n",
      "Correction:\n"
     ]
    }
   ],
   "source": [
    "for premise, hypothesis in pairs:\n",
    "    prompt = f\"<pad> Determine if the hypothesis is true given the premise?\\n\\nPremise: {premise}\\n\\nHypothesis: {hypothesis}\"\n",
    "    result = classifier(prompt)[0]  # Assuming single result\n",
    "\n",
    "    if result['label'] != 'consistent':\n",
    "        print(f\"Hallucination detected for: {premise}\")\n",
    "        print(f\"Original hypothesis: {hypothesis}\")\n",
    "\n",
    "        correction_prompt = f\"Given the following premise, suggest a correction for the hypothesis if it contains inaccuracies.\\n\\nPremise: {premise}\\n\\nHypothesis: {hypothesis}\\n\\nCorrection:\"\n",
    "        correction = correction_generator(correction_prompt, max_length=50)[0]['generated_text']\n",
    "        \n",
    "        print(f\"Suggested correction: {correction}\")\n",
    "    else:\n",
    "        print(f\"No hallucination detected for: {premise}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
